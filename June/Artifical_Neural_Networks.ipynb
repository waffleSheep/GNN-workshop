{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcOz5aRgy3gt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Torch Tensors"
      ],
      "metadata": {
        "id": "luNqfJN9i7o2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensors are n-dimenstional arrangements of object\n",
        "Vector is first order tensor, matrix is a second order tensor."
      ],
      "metadata": {
        "id": "DmThxcwKjLGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectors"
      ],
      "metadata": {
        "id": "aesBUE9PjGGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\mathbf{y}=\n",
        "\\begin{bmatrix}\n",
        "9\\\\\n",
        "4\\\\\n",
        "24\\\\\n",
        "-12\n",
        "\\end{bmatrix}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "7cAupZ4LjZyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.tensor([9,4,24,-12]).float()\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS3KQDkgjGUt",
        "outputId": "2dd3b9aa-9721-4f32-f86d-5a8c662e481f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  9.,   4.,  24., -12.])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multidimensional tensor"
      ],
      "metadata": {
        "id": "zzMPXiJvjEG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "A=\n",
        "\\begin{bmatrix}\n",
        "1 & 3 & 2 & -1\\\\\n",
        "5 & 2 & 1 & -2\\\\\n",
        "0 & 1 & 2 & 4\\\\\n",
        "1 & 1 & -1 & -3\n",
        "\\end{bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "1DLzva0ejBaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.tensor([[1,3,2,-1],[5,2,1,-2],[0,1,2,4],[1,1,-1,-3]]).float()\n",
        "A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klYkc5lCi6R0",
        "outputId": "a886d0a0-d414-455d-d327-bf89893e0bc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.,  3.,  2., -1.],\n",
              "        [ 5.,  2.,  1., -2.],\n",
              "        [ 0.,  1.,  2.,  4.],\n",
              "        [ 1.,  1., -1., -3.]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numpy arrays to torch tensors"
      ],
      "metadata": {
        "id": "_lBlPvMQjgKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B = np.array([[1,0],[0,1]])\n",
        "torch.tensor(B).float()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuXEH2N-jAwd",
        "outputId": "f68c5862-30c7-4f7c-f5fb-b14e4ba89a80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0.],\n",
              "        [0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Neural Network models with torch"
      ],
      "metadata": {
        "id": "VJccLwP4jvzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### General Structure"
      ],
      "metadata": {
        "id": "9D-8hZQvj_Td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Ne(nn.Module):\n",
        "    # Intialise your trainable layers here\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    # Define a forward pass with your trainable and functional layers here\n",
        "    def forward(self, x):\n",
        "        pass"
      ],
      "metadata": {
        "id": "1OQQupEfkyE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Model for Mnist Dataset"
      ],
      "metadata": {
        "id": "UNQIhdUvkyZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 1 channel 28 x 28 -> 10 channels 24 x 24, 10 filters, 5 by 5, 250 trainable params\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        # 10 channel 12 x 12 -> 20 channel 8 x 8, 200 filters, 5 by 5, 5000 trainable params\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        # 320 neurons -> 50 neurons, 16000 trainable params + 50\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        # 50 neurosn -> 10 neurons, 500 trainable params + 10\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)"
      ],
      "metadata": {
        "id": "zkPrG8oxjpF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = Net()\n",
        "optimizer = optim.SGD(network.parameters(), lr=0.01, momentum=0.9)"
      ],
      "metadata": {
        "id": "5eNuv8ozo_ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset loading"
      ],
      "metadata": {
        "id": "s4Oy14PyoTA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/handwritten-digit-mnist-pytorch-977b5338e627"
      ],
      "metadata": {
        "id": "n0e2Nxv4or5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))])\n",
        "trainset = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
        "valset = datasets.MNIST('data', train=False, transform=transform)"
      ],
      "metadata": {
        "id": "k8cGkKOBlTC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=640, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(valset, batch_size=640, shuffle=True)"
      ],
      "metadata": {
        "id": "gvTAq5V3iqX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "PTQqEGDIo1hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network.train()\n",
        "for epoch in range(3):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = network(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9HCwtbJr1qP",
        "outputId": "11689b35-5098-4c1d-f903-355df96fa992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-61-6c2218f37724>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.393257\n",
            "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.363937\n",
            "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.383027\n",
            "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.317677\n",
            "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.385315\n",
            "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.410912\n",
            "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.435674\n",
            "Train Epoch: 0 [44800/60000 (74%)]\tLoss: 0.343292\n",
            "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.345185\n",
            "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.300280\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.344296\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.353807\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.292317\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.376967\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.419868\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.281213\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.357538\n",
            "Train Epoch: 1 [44800/60000 (74%)]\tLoss: 0.299612\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.342457\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.249130\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.372748\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.299857\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.302661\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.280219\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.276044\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.267033\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.294122\n",
            "Train Epoch: 2 [44800/60000 (74%)]\tLoss: 0.281251\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.342067\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.281761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation"
      ],
      "metadata": {
        "id": "P-UZmh_MuUTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = network(data)\n",
        "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_GllbBdo21B",
        "outputId": "f33eb6db-b862-47e4-af91-c6f71e0dd79c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-61-6c2218f37724>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Avg. loss: 0.0942, Accuracy: 9708/10000 (97%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f9_CsxuQjcGx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}